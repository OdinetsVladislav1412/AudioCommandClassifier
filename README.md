<h1 align="center">AudioCommandClassifier</h1>

<h2 align="center">1. Введение</h2>
<p align="justify">
Программа предназначена для построения пользовательского классификатора слов на основе звуковых признаков и классических алгоритмов машинного обучения. Она обеспечивает сбор аудиоданных, автоматическую обработку сигнала, извлечение информативных признаков и обучение моделей для последующего распознавания произнесённых слов. Такой инструмент решает задачу разработки компактной системы распознавания речи в узкоспециализированном словаре.
</p>



<h2 align="center">2. Руководство пользователя</h2>

<h3 align="center">2.1. Графический интерфейс</h3>

<p align="center">
<img width="90%" alt="{997A15DD-4F63-467A-934A-5E92F66F4C6B}" src="https://github.com/user-attachments/assets/1674281a-d39f-41a1-b098-bde4d47dc5ab" />
</p>

### Левая панель
- Запись слова:
	- Поле ввода: Введите слово для записи.
	- Кнопка "Записать образец (3 сек)": Запускает 3-секундную запись.
	- Таймер записи: Показывает обратный отсчет (00:03).
	- Статус: Отображает текущее состояние.
	- Прогресс-бар: Анимируется во время записи.

- Настройки обработки звука:
	- Слайдер "Уровень шумоподавления": От 0.1 до 1.0 (по умолчанию 0.7). Увеличивает/уменьшает агрессивность удаления шума.
	- Слайдер "Порог обнаружения речи (dB)": От -60 до -20 (по умолчанию -40). Определяет уровень, ниже которого звук считается молчанием.

- Выбор модели классификации:
	- Радио-кнопки: Random Forest (по умолчанию), CatBoost (если установлен), Logistic Regression.
	- Если CatBoost не установлен, отображается предупреждение "CatBoost не установлен".

- Настройки валидации:
	- Чекбокс "Использовать кросс-валидацию": Включено по умолчанию.
	- Спинбокс "Количество фолдов": От 3 до 10 (по умолчанию 5).

- Управление моделью:
	- Кнопка "Обучить модель": Обучает модель на накопленных данных.
	- Кнопка "Тестировать запись": Записывает 3-секундный тестовый образец и распознает слово.
	- Таймер тестирования: Обратный отсчет (00:03).
	- Кнопка "Очистить все данные": Удаляет все записи и модель.
	- Кнопка "Удалить последнюю запись": Удаляет последний добавленный образец.
	- Кнопка "Загрузить датасет": Загружает существующие файлы из папки data.

- Визуализация аудиозаписи:
	- График сверху: Исходная запись (синий цвет).
	- График снизу: После обработки (зеленый цвет, с удалением шума и молчания).
	- Оси: Амплитуда vs. Время (сэмплы).


### Правая панель

- Статистика записей:
	- Текстовое поле: Показывает количество записей по словам, общее количество, уникальные слова, выбранную модель, статус обучения и метрики (если модель обучена).

- Распределение по классам:
	- Гистограмма: Столбцы для каждого слова с количеством записей.



<h3 align="center">2.2. Запуск программы</h3>

<p align="justify">
Чтобы запустить программу, зайдиет в дирректорию программы и в папке <code>dict</code> выберете файл <code>main.exe</code>.
</p>

<p align="justify">
При запуска откроется главное окно с заголовком "Классификатор речевых команд" с выставленным по умолчанию значениями. Записанные ранее данные автоматически загружаются из папки <code>data</code>: все WAV-файлы, находящиеся в подпапках (имена подпапок = названия слов/классов), сразу добавляются в датасет, извлекаются признаки, обновляется статистика и графики — вам не нужно вручную нажимать кнопку «Загрузить датасет», всё происходит сразу после старта программы. Удалить все записанные данные можно кнопкой "Очистить все данные"
</p>

<p align="center">
<img width="90%" alt="{8DD0FE07-1AFD-4D1E-A226-F7088FA7502C}" src="https://github.com/user-attachments/assets/e9b84b5c-e996-4e92-9226-7f1e5dfff0d4" />
</p>



<h3 align="center">2.3. Запись образцов</h3>

### Чтобы добавить новые образцы для обучения модели:
- Введите слово (класс) в поле ввода "Слово для записи".
- Нажмите кнопку "Записать образец (3 сек)".
- Говорите в микрофон в течение 3 секунд — таймер покажет обратный отсчет, а прогресс-бар будет анимирован.
- После завершения записи аудио автоматически сохраняется в папке <code>data/слово/слово_дата.wav</code>.
- Визуализация обновится: верхний график покажет исходную запись, нижний — после обработки (удаление шума и молчания).
- Статистика и гистограмма в правой панели обновятся автоматически.

<p align="center">
<img width="90%" alt="{5EFC9CA7-4B8C-46F7-B27D-58A5E7D6030D}" src="https://github.com/user-attachments/assets/49f88c37-be82-4ac9-91f4-9b8af1765dd9" />
</p>

<h3 align="center">2.4. Настройка обработки аудио</h3>

### Перед записью или обучением настройте параметры для оптимальной обработки:


- Переместите слайдер "Уровень шумоподавления" (0.1–1.0): Более высокие значения агрессивнее удаляют шум, но могут искажать речь. По умолчанию 0.7 — баланс для типичных условий.
- Переместите слайдер "Порог обнаружения речи (dB)" (-60 до -20): Более низкие значения (например, -50) лучше захватывают тихую речь, но могут включать шум; более высокие (например, -30) строже отсекают молчание. По умолчанию -40 подходит для большинства случаев.



<h3 align="center">2.5. Выбор модели классификации</h3>

### Выберите тип модели для обучения:

- Random Forest (по умолчанию): Хорош для небольших датасетов, устойчив к переобучению, не требует много данных.
- CatBoost: Более точная градиентная бустинг-модель, эффективна для аудио-признаков.
- Logistic Regression: Простая линейная модель, быстрая, но может быть менее точной на сложных данных.


<h3 align="center">2.6. Настройки валидации</h3>

### Для оценки качества модели:


- Включите чекбокс "Использовать кросс-валидацию" (рекомендуется для надежности).
- Укажите количество фолдов в спинбоксе (3–10, по умолчанию 5): Больше фолдов — точнее оценка, но дольше обучение.

- Кросс-валидация использует StratifiedKFold для баланса классов. Если данных мало (< 2 * фолды), она отключится автоматически.


<h3 align="center">2.7. Обучение модели</h3>

- Нажмите "Обучить модель".
- Процесс: Извлечение признаков, разделение на train/test (80/20), обучение, расчет метрик (accuracy, precision, recall, F1).
- Если включена кросс-валидация: Дополнительно вычисляются средние метрики с стандартным отклонением.
- После обучения откроется окно с детальными метриками: время обучения, метрики на тесте, кросс-валидация (если включена) и classification report.
- Статистика в правой панели обновится с метриками и статусом "Модель обучена".
- Если данных недостаточно или классов меньше 2, отобразится ошибка. Обучение занимает секунды на малых датасетах.

<p align="center">
<img width="70%" alt="{C12CC03B-2C27-424F-BBE5-652A30E5F43F}" src="https://github.com/user-attachments/assets/9cccdd4e-02e8-40de-aeb2-60349772a506" />
</p>

<h3 align="center">2.8. Тестирование модели</h3>

### После обучения протестируйте распознавание:
- Нажмите "Тестировать запись".
- Говорите в микрофон 3 секунды (таймер и прогресс-бар активируются).
- Результат: Всплывающее окно с распознанным словом и типом модели. Статус обновится, визуализация покажет тестовую запись.
- Если модель не обучена, отобразится ошибка. Тестируйте в реальных условиях для проверки точности.


<h3 align="center">2.9. Управление даннымии</h3>

### Для работы с датасетом:
- Удалить последнюю запись: Нажмите "Удалить последнюю запись" — удалит последний образец из памяти и с диска, обновит статистику.
- Очистить все данные: Нажмите "Очистить все данные" — удалит все записи из памяти (файлы на диске останутся), сбросит модель и графики. Подтверждение требуется.
- Загрузить датасет: Нажмите "Загрузить датасет" — перезагрузит все WAV-файлы из data, если вы добавили их вручную.


<h3 align="center">2.10. Сохранение и загрузка</h3>

- Записи сохраняются автоматически в <code>data/класс/файл.wav</code>.
- Модель хранится только в памяти — при перезапуске обучите заново.
- Для импорта внешних аудио: Поместите WAV-файлы в data/класс/ и нажмите "Загрузить датасет" (или перезапустите приложение для автозагрузки).

<h2 align="center">3. Архитектура приложения</h2>

<h3 align="center">3.1. Структура директории проекта</h3>

<p align="justify">
Корневая директория проекта включает исходные модули, вспомогательные каталоги и элементы, необходимые для сборки, запуска и воспроизведения программной среды. Структура организована таким образом, чтобы обеспечить логическое разделение данных, кода и артефактов сборки.
</p>

<ul>
<li><code>__pycache__</code> — автоматически создаваемый каталог, содержащий байт-код Python, что ускоряет последующие запуски.</li>
<li><code>build</code> — временные данные, формируемые в процессе сборки исполняемых модулей.</li>
<li><code>data</code> — хранилище аудиозаписей, распределённых по лексическим классам.  
  <ul>
    <li>Подкаталоги представляют собой названия слов или классов.</li>
    <li>Файлы внутри — WAV-записи с временными метками.</li>
  </ul>
</li>
<li><code>dist</code> — директория с готовыми дистрибутивами, включая исполняемые файлы формата EXE.</li>
<li><code>venv</code> — виртуальное окружение, содержащее изолированные зависимости.</li>
</ul>

<p align="justify">
В корневой директории также располагаются ключевые файлы приложения:
</p>

<ul>
<li><code>audio_app.py</code> — реализация графического интерфейса и управляющей логики программы.</li>
<li><code>audio_classifier.py</code> — модули классификации аудиосигналов и формирования признаков.</li>
<li><code>main.py</code> — точка входа, инициирующая запуск пользовательского интерфейса.</li>
<li><code>main.spec</code> — конфигурационный файл PyInstaller, определяющий параметры сборки.</li>
<li><code>requirements.txt</code> — формализованный перечень зависимостей для точного воспроизведения окружения.</li>
</ul>

<h3 align="center">3.2. Используемые библиотеки</h3>

<p align="justify">
Полный перечень зависимостей определён в файле <code>requirements.txt</code>, что гарантирует корректное восстановление программной среды. Ниже перечислены ключевые библиотеки и их функциональное назначение.
</p>

<ul>
<li><b>librosa</b> — анализ аудиосигналов, извлечение спектральных и энергетических характеристик, подавление тишины и шумов.</li>
<li><b>scikit-learn</b> — модели и методы классического машинного обучения.</li>
<li><b>tkinter</b> — инструменты разработки графического интерфейса.</li>
<li><b>numpy</b> — высокопроизводительные численные операции.</li>
<li><b>matplotlib</b> — визуализация сигналов и данных.</li>
<li><b>sounddevice</b> — запись аудио через микрофон.</li>
<li><b>catboost</b> — метод градиентного бустинга, применяемый для задач классификации.</li>
<li><b>scipy</b> — вычислительные методы и алгоритмы обработки сигналов.</li>
<li><b>soundfile</b> — сохранение аудиофайлов в формате WAV.</li>
<li><b>threading</b> и <b>time</b> — управление параллельными процессами и временными интервалами.</li>
<li><b>os</b> и <b>datetime</b> — операции с файловой системой и временными метками.</li>
</ul>

<h3 align="center">3.3. Описание Python-файлов</h3> 

### audio_classifier.py

<p align="justify"> 
Модуль, содержащий ключевой класс <code>AudioClassifier</code>, который отвечает за предобработку аудиосигналов, извлечение признаков, управление сэмплами, обучение модели и получение предсказаний. 
</p>

#### Основные методы:

- Предобработка аудио:
	- <code>remove_silence_advanced</code> — удаление тишины с использованием порогов громкости
	- <code>apply_noise_reduction</code> — спектральное шумоподавление

- Извлечение признаков:
	- <code>extract_features</code> — формирование вектора длиной 65 элементов, включающего MFCC, chroma, RMS и другие характеристики с обработкой NaN

- Работа с сэмплами:
	- <code>add_sample</code> — добавление нового сэмпла
	- <code>remove_last_sample</code> — удаление последнего сэмпла

- Обучение модели:
	- <code>train</code> — кросс-валидация, вычисление метрик, обучение временной модели

- Предсказание:
	- <code>predict</code> — класс по аудиозаписи

### main.py 

<p align="justify"> 
Точка входа приложения. Запускает графический интерфейс, создаёт окно Tkinter, инициализирует объект <code>AudioRecorderApp</code> и запускает цикл <code>mainloop</code>. Является минималистичным launcher-файлом, удобным для сборки в EXE через PyInstaller. </p> 

### audio_app.py
<p align="justify"> 
Главный модуль пользовательского интерфейса, содержащий класс <code>AudioRecorderApp</code>, управляющий логикой записи, обработкой аудио, обучением модели и отображением визуализаций. </p>

#### Основные методы:

- Интерфейс:
	- <code>setup_ui</code> — создание фреймов интерфейса: запись, настройки и визуализация

- Запись и тест:
	- <code>record_sample</code> — запись образца с таймером
	- <code>test_recording</code> — запись для тестирования модели, многопоточная реализация

- Визуализация:
	- <code>visualize_audio</code> — построение графиков аудиосигнала до и после обработки

- Обучение:
	- <code>train_model</code> — вывод метрик обучения в модальном окне

- Статистика:
	- <code>update_stats</code> — обновление текстовой статистики и гистограммы классов

<h2 align="center">4. Машинное обучение</h2>

<p align="justify">
Модуль машинного обучения реализован в классе <code>AudioClassifier</code> из файла <code>audio_classifier.py</code> и ориентирован на supervised learning для классификации речевых команд. 
Данные представляют собой 3-секундные аудиозаписи, преобразованные в векторы признаков. 
Цель — точная классификация слов с учетом обработки шумных сигналов. 
Класс интегрирован в графический интерфейс и используется для добавления сэмплов, обучения и предсказания.
</p>

<h3 align="center">4.1. Обработка данных и извлечение признаков</h3>

### Источник данных:
<p align="justify">
Аудиозаписи создаются микрофоном c частотой 22050 Гц в формате WAV и сохраняются в папке <code>data</code> в подпапках по классам. Для обучения требуется не менее шести записей, что соответствует минимум двум классам.
</p>

### Предобработка аудио:
<p align="justify">

#### 1. Удаление тишины — remove_silence_advanced:
Анализируется энергия RMS во временных окнах 25 мс с шагом 10 мс. 
Используется динамический порог на основе медианы уровня шума или фиксированный уровень минус 40 дБ. Учитываются минимальная длительность речи 0.3 секунды и минимальная длительность тишины 0.2 секунды. Если итоговый сегмент слишком короткий, используется исходная запись.
</p>

<p align="justify">

#### 2. Шумоподавление — apply_noise_reduction:
Проводится спектральный анализ STFT. Оценка шума вычисляется по первым 10 процентов фреймов (не менее трети сигнала). Выполняется спектральное вычитание с уровнем 0.7 по умолчанию. Амплитуда нормализуется для устойчивости обработки.
</p>

<p align="justify">

#### 3. Извлечение признаков — extract_features:
Используется библиотека librosa.  
Формируется вектор из 65 чисел типа float64:<br>

- MFCC: 26 значений (средние и стандартные отклонения 13 коэффициентов)<br>
- Chroma STFT: 24 значения<br>
- Spectral contrast: 7 значений<br>
- RMS, spectral centroid, spectral rolloff, zero crossing rate, mel spectrogram: по 2 значения <br>
Все значения проходят обработку NaN → 0.  
Этот вектор используется в классических моделях sklearn.
</p>

### Дополнительно:
<p align="justify">
Данные хранятся в памяти в списках features и labels.  
Удаление последней записи выполняется функцией <code>remove_last_sample</code>.  
Маштабирование признаков не применяется — модели работают со значениями напрямую.
</p>

<h3 align="center">4.2. Доступные модели и их настройка</h3>

<p align="justify">
Выбор модели задается через метод <code>set_model_type</code> в интерфейсе.  
Доступные варианты: random forest (тип по умолчанию), catboost и logistic regression.
</p>

<p align="justify">
<b>RandomForestClassifier</b><br>
Количество деревьев — 100, фиксированное начальное состояние random state 42.  
Подходит для небольших датасетов и устойчив к шумовым данным.
</p>

<p align="justify">
<b>CatBoostClassifier</b><br>
Используются параметры: 100 итераций, скорость обучения 0.1, глубина деревьев 6.  
Даёт высокую точность на малом объёме данных.
</p>

<p align="justify">
<b>LogisticRegression</b><br>
Используется L2 регуляризация и до 1000 итераций оптимизации.  
Подходит как простая интерпретируемая модель.
</p>

<h3 align="center">4.3. Кросс-валидация</h3>

<p align="justify">
Кросс-валидация включается через параметр интерфейса.  
Используется метод cross_validate со стратифицированным разбиением StratifiedKFold.  
Количество фолдов - по умолчанию 5, можно изменить от 3 до 10.  
Вычисляются accuracy, precision weighted, recall weighted и f1 weighted.
</p>

<p align="justify">
При достаточном количестве данных сохраняются средние значения и стандартные отклонения метрик.  
Если классов меньше двух или данных мало, кросс-валидация отключается.
</p>

<h3 align="center">4.4. Процесс обучения и метрики</h3>

<p align="justify">
<b>Процесс обучения - train</b><br>
Проверяется наличие двух и более классов.  
Если кросс-валидация включена, сначала вычисляются её метрики.  
После этого модель обучается на всех данных.  
Дополнительно выполняется разделение на train и test в пропорции 80 к 20 для вычисления контрольных метрик.
</p>




<b>Основные метрики: - train</b><br>
<p align="justify">

- accuracy;
- precision weighted;
- recall weighted;
- f1 weighted;
- а также подробный classification report по классам.</p>

Метрики отображаются в модальном окне интерфейса и в панели статистики.

<h2 align="center">5. Заключение</h2>

<p align="justify">
Разработанная программа <b>AudioCommandClassifier</b> представляет собой полнофункциональное решение для создания пользовательских систем распознавания речевых команд на основе классических методов машинного обучения. Приложение успешно сочетает в себе простоту использования через интуитивный графический интерфейс с мощным функционалом обработки аудиосигналов и построения классификационных моделей.
</p>

<p align="justify">
<b>Ключевые преимущества системы:</b>
</p>

<ul>
<li><b>Универсальность применения</b> - программа позволяет создавать специализированные системы распознавания для узких предметных областей с пользовательским словарем команд</li>
<li><b>Автоматизация процессов</b> - от записи аудио до извлечения признаков и обучения модели все этапы максимально автоматизированы</li>
<li><b>Гибкость настройки</b> - регулируемые параметры обработки звука и выбора алгоритмов машинного обучения позволяют адаптировать систему под различные условия эксплуатации</li>
<li><b>Простота развертывания</b> - наличие исполняемого файла исключает необходимость установки дополнительного программного обеспечения</li>
<li><b>Визуализация процессов</b> - графики аудиосигналов и статистики помогают пользователю контролировать качество записей и работу системы</li>
</ul>

<p align="justify">
<b>Области практического применения:</b>
</p>

<ul>
<li>Системы голосового управления специализированным оборудованием</li>
<li>Образовательные проекты по изучению обработки сигналов и машинного обучения</li>
<li>Прототипирование систем распознавания речи для IoT-устройств</li>
<li>Создание accessibility-решений для людей с ограниченными возможностями</li>
<li>Исследовательские задачи в области цифровой обработки аудиосигналов</li>
</ul>

<p align="justify">
<b>Ограничения и перспективы развития:</b>
</p>

<p align="justify">
Текущая реализация демонстрирует высокую эффективность на ограниченном словаре команд (до 10-15 слов) при условии наличия достаточного количества обучающих примеров. Однако для промышленного применения в условиях сильных шумов или большого словаря рекомендуется рассмотреть переход на нейросетевые подходы, такие как сверточные или рекуррентные нейронные сети.
</p>

<p align="justify">
Программа может быть расширена за счет реализации дополнительных функций: сохранения/загрузки обученных моделей, интеграции с облачными сервисами, добавления методов аугментации данных для улучшения обобщающей способности моделей.
</p>

<p align="justify">
В целом, <b>AudioCommandClassifier</b> представляет собой законченное и готовое к использованию решение, которое демонстрирует эффективность классических подходов к машинному обучению в задачах распознавания речевых команд и служит отличной отправной точкой для более сложных проектов в области обработки аудиосигналов.
</p>
